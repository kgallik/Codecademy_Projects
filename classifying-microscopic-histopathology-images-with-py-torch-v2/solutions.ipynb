{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc16985f-ec8c-434c-90b6-54227e04e2ea",
   "metadata": {},
   "source": [
    "# Classifying Microscopic Histopathology Images with PyTorch\n",
    "\n",
    "- [View Solution Notebook](./solutions.html)\n",
    "- [View Project Page](https://www.codecademy.com/content-items/b68fc937824d450e8bce11e24126e1e8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2c653-6127-4917-9084-de9f46c606d3",
   "metadata": {},
   "source": [
    "**Setup - Libraries and Custom PCam Dataset Loading Class**\n",
    "\n",
    "Run the cell below to import the libraries and the custom PCam dataset loading class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba83e27-89d4-459f-8eda-5045f5f6f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class PCamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading the microscopic histopathology images within the PCam dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, transform=None, num_samples=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            num_samples (int, optional): Number of samples to load. If None, loads all samples\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        if num_samples is not None:\n",
    "            self.annotations = self.annotations.head(num_samples)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        # Get image path and label\n",
    "        img_path = self.annotations.iloc[idx, 0]\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "        \n",
    "        # Load and convert image\n",
    "        image = Image.open(img_path)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Convert label to float\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17292ac3-37e5-47c3-8dc7-4ecc9b4063f8",
   "metadata": {},
   "source": [
    "## Task Group 1 - CNN Pre-Processing\n",
    "\n",
    "Let's first train a CNN model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8adfaec-b2b0-458f-8e7c-f8a17206eccf",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Let's use `transforms.Compose([])` to create the image pre-processing pipeline to apply the following transformations **and** augmentations to the training set:\n",
    "- resizes images to `96`x`96` pixels\n",
    "- randomly flips images horizontally\n",
    "- randomly rotates images `15` degrees clockwise or counterclockwise\n",
    "- adjusts brightness and contrast within 20% (or `0.20`)\n",
    "- converts the image datatype to a PyTorch tensor with values ranging from `[0.0, 1.0]`\n",
    "- normalize the pixel values within the **3** color channels to have a mean of `0.5` and a standard deviation of `0.5`\n",
    "\n",
    "Save the training set pre-processing pipeline to the variable `train_transform`.\n",
    "\n",
    "**Note**: be sure to apply the transformations in the correct order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d39828-d35c-45c2-9905-e51ffe62fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a4210-0f88-414c-9213-9945013d938d",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Next, load the PCam **training set** while applying the training transformation pipeline to each image using the custom `PCamDataset` class with the following parameters:\n",
    "- `csv_file` to specify the training set located in the path `'data/train_labels.csv'`\n",
    "- `transform` to apply the `train_transform` pre-processing pipeline\n",
    "\n",
    "Save the PCam training set to the variable `train_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275a16fa-6b83-41b1-a4b2-de73ed537fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image: tensor([[[-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         ...,\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137]],\n",
      "\n",
      "        [[-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         ...,\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137]],\n",
      "\n",
      "        [[-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         ...,\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137],\n",
      "         [-0.9137, -0.9137, -0.9137,  ..., -0.9137, -0.9137, -0.9137]]])\n",
      "First label: tensor(1.)\n",
      "Training size: 600\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataset = PCamDataset(\n",
    "    csv_file='data/train_labels.csv',\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# first training image\n",
    "image, label = train_dataset[0]\n",
    "print(\"First image:\", image)\n",
    "print(\"First label:\", label)\n",
    "print(\"Training size:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484e8e1-8d30-4d12-80d3-08f26352cfc7",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Create an iterable using the PyTorch `DataLoader` utility class that allows us to efficiently load the training set images in batches during training:\n",
    "\n",
    "- set to load `8` training images per batch\n",
    "- be sure to shuffle the training images\n",
    "\n",
    "Save the dataloader iterable to the variable `train_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "137129f4-9f5d-4bfb-ba97-f8b121092589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([8, 3, 96, 96])\n",
      "Training labels: tensor([1., 1., 0., 1., 0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# show output - DO NOT MODIFY\n",
    "first_batch = next(iter(train_dataloader))\n",
    "images, labels = first_batch\n",
    "print(\"Batch shape:\", images.shape)\n",
    "print(\"Training labels:\", labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c61af-5ff8-47c9-83d2-2c06530f95cf",
   "metadata": {},
   "source": [
    "### Optional Task\n",
    "\n",
    "Visualize the images in the first training batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608422eb-6361-4cc0-9d93-11aad13d25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first training batch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of training data\n",
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)\n",
    "batch_size = 8\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(' '.join('%5s' % labels[j].item() for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd0803-4a69-4eb1-8407-46f744ee19aa",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Next, let's load and pre-process the PCam **validation** and **testing** set.\n",
    "\n",
    "Remember, we don't need to apply the augmentation techniques used to pre-process the training set. But we do need to apply the same transformations (resizing, normalization, and tensor conversion) to ensure the testing set images are consistent with the training set images and are suitable for use as model inputs.\n",
    "\n",
    "**A.** Create the image pre-processing pipeline using the `transforms.Compose([])` class from the `torchvision` library that applies the following transformations to the testing set:\n",
    "- resizes images to `96`x`96` pixels\n",
    "- converts the image datatype to a PyTorch tensor with values ranging from `[0.0, 1.0]`\n",
    "- normalize the pixel values within the 3 color channels to have a mean of `0.5` and a standard deviation of `0.5`\n",
    "\n",
    "Save the validation/testing pre-processing pipeline to the variable `val_test_transform`.\n",
    "\n",
    "**Note**: be sure to apply the transformations in the correct order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee7571-49a1-423d-b616-a49308038207",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379cbd2-53f5-49de-98c9-6be6312b7afa",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "**A.** Next, load the PCam **validation set** while applying the validation/testing pre-processing pipeline:\n",
    "- `csv_file` to specify the validation set located in the path `'data/validation_labels.csv'`\n",
    "- `transform` to apply the `val_test_transform` pre-processing pipeline\n",
    "\n",
    "Save the loaded validation set to the variable `val_dataset`.\n",
    "\n",
    "**B.** Lastly, create an iterable using the PyTorch `DataLoader` utility class that allows us to efficiently load the validation set images in batches during evaluation:\n",
    "\n",
    "- set to load `32` images per batch\n",
    "- be sure **not** to shuffle the images\n",
    "\n",
    "Save the dataloader iterable to the variable `val_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8457913-a888-45e0-9866-dd48d9d05499",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PCamDataset(\n",
    "    csv_file='data/validation_labels.csv',\n",
    "    transform=val_test_transform\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False\n",
    ")\n",
    "print(\"Validation size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d11ae01-d3d6-4955-8e07-7c75373b8e2d",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "**A.** Next, load the PCam **testing set** while applying the validation/testing pre-processing pipeline:\n",
    "- `csv_file` to specify the validation set located in the path `'data/test_labels.csv'`\n",
    "- `transform` to apply the `val_test_transform` pre-processing pipeline\n",
    "\n",
    "Save the loaded validation set to the variable `test_dataset`.\n",
    "\n",
    "**B.** Lastly, create an iterable using the PyTorch `DataLoader` utility class that allows us to efficiently load the testing set images in batches during evaluation:\n",
    "\n",
    "- set to load `32` images per batch\n",
    "- be sure **not** to shuffle the images\n",
    "\n",
    "Save the dataloader iterable to the variable `test_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc16cf2-6a1b-4d33-b548-74312dcb6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PCamDataset(\n",
    "    csv_file='data/test_labels.csv',\n",
    "    transform=val_test_transform\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False\n",
    ")\n",
    "print(\"Testing size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf2a0a-9776-4ab7-83e6-d625f42518d6",
   "metadata": {},
   "source": [
    "## Task Group 2 - CNN Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7075e8-9ce7-4afc-af33-e79a936b6c0e",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Let's now a create a CNN architecture as a class named `SimpleCNN` using the `nn.Module`.\n",
    "\n",
    "**A.** Define the `__init__` method with the following CNN layers:\n",
    "1. `self.conv1` is the first convolutional layer with `3` input channels, `32` output channels, `3x3` filter sizes, and `1` padding\n",
    "2. `self.conv2` is the second convolutional layer with `32` input channels, `64` output channels, `3x3` filter sizes, and `1` padding\n",
    "3. `self.conv3` is the third convolutional layer with `64` input channels, `128` output channels, `3x3` filter sizes, and `1` padding \n",
    "4. `self.fc1` is the first fully connected layer with `18432` input nodes and `256` output nodes\n",
    "    - `18432` corresponds to the length of the flattened 1-D vector after last max pooling layer (`128 x 12 x 12 = 18432`)\n",
    "5. `self.fc2` is the second fully connected layer with `256` input nodes and `1` output node\n",
    "\n",
    "**B.** Define the `forward` method that processes each image `x` with the forward operations in following order:\n",
    "\n",
    "1. Pass the image through the first convolutional layer and then apply the ReLU activation function\n",
    "2. Pass the first activated convoluted output to a max pooling layer with a 2x2 filter\n",
    "3. Pass the first max pooling output to the second convolutional layer and then apply the ReLU activation function\n",
    "4. Pass the second activated convoluted output to a 2nd max pooling layer with a 2x2 filter\n",
    "5. Pass the second max pooling output to the third convolutional layer and then apply the ReLU activation function\n",
    "6. Pass the third activated convoluted output to a 3rd max pooling layer with a 2x2 filter\n",
    "7. Flatten the third max pooling output into a tensor (with batch size)\n",
    "8. Pass the flattened tensor to the first fully connected layer and then apply the ReLU activation function\n",
    "9. Pass the activated output to the second fully connected layer through a Sigmoid activation function\n",
    "    - **Hint:** Use `x = torch.sigmoid(self.fc2(x)).squeeze(1)`\n",
    "10. Return Sigmoid activated output\n",
    "\n",
    "**C.** Create an instance of the CNN model class and save it to the variable `cnn_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56456311-31c7-436a-bf04-6a63e16d2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x)).squeeze(1)\n",
    "        return x\n",
    "        \n",
    "# Instantiate the model\n",
    "cnn_model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb7e21-f261-4d99-b7e9-c768d6b41211",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Set the CNN hardware device to GPU/CPU.\n",
    "\n",
    "**A.** Create the `device` variable that detects whether GPU (`'cuda'`) or CPU is available.\n",
    "- use `torch.device()` to detect the device\n",
    "- use `torch.cuda.is_available()` to check if GPU is available\n",
    "    - if GPU is available, return the string `'cuda'`\n",
    "    - if not, return the string `'cpu'`\n",
    "\n",
    "**B.** Move the `cnn_model` to the available device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851e43d-e6b6-4028-8458-980ad666fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707a95e-d73e-442e-b980-b9dab910541e",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Let's initialize the loss function and optimizer for training using the `torch.optim` module.\n",
    "\n",
    "**A.** Create an instance of the binary-entropy loss function `nn.BCELoss()` in PyTorch and save it to the variable `criterion`.\n",
    "\n",
    "**B.** Create an instance of the Adam optimizer in PyTorch with a learning rate of `0.0005` and save it to the variable `optimizer`.\n",
    "- be sure to optimize the parameters in the CNN model we instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bc867-f34c-46f4-8c8e-6437fab0c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.0005)\n",
    "\n",
    "# show output\n",
    "print(\"Loss function:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3968c-0aaf-4d08-abaf-7b85c2ad345a",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Now, let's create a training loop that:\n",
    "- trains the CNN model on training set and keeps track of the training loss\n",
    "- keeps track of the validation loss on the training set\n",
    "\n",
    "**A.** Initialize the following empty lists to keep track the training and validation losses per epoch:\n",
    "- `train_losses` to keep track of the training loss\n",
    "- `val_losses` to keep track of the validation loss\n",
    "\n",
    "**B.** Train the CNN on `5` epochs by assigning the value `5` to the variable `num_epochs`.\n",
    "\n",
    "**C.** Initialize the following variables to `0` to keep track of during training:\n",
    "- `total_train_loss` to keep track of the total training loss\n",
    "- `total_val_loss` to keep track of the total validation loss\n",
    "\n",
    "**D.** Build the training section:\n",
    "- loop through the training batch images and labels\n",
    "- within each training batch:\n",
    "    - place the images to the GPU device \n",
    "    - reset the gradients to zero\n",
    "    - pass the batch through the CNN forward pass\n",
    "    - calculate the training loss\n",
    "    - backpropagate the loss\n",
    "    - adjust the weights and biases\n",
    "    - update the total training loss\n",
    "\n",
    "**E.** Within the validation section, evaluate the validation set performance:\n",
    "- loop through the validation batch images and labels\n",
    "- within each validation batch:\n",
    "    -  place the images to the GPU device\n",
    "    -  pass the batch through the CNN forward pass\n",
    "    -  calculate the validation loss\n",
    "    -  update the total validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c165c-86e6-4897-89bc-65cf914168a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training\n",
    "    cnn_model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = cnn_model(images)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    # DO NOT MODIFY\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    # Save the average training and validation losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Print training and validation losses\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b92a4f-516a-4b86-b2da-a040e5109fab",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Hint: Tracking the training and validation losses</summary>\n",
    "    \n",
    "You can add this code snippet to the training loop to track the training and validation losses:\n",
    "\n",
    "```py\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    # Save the average training and validation losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Print training and validation losses\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39daf771-f520-4582-8d51-3719ca26d794",
   "metadata": {},
   "source": [
    "### Optional Task\n",
    "\n",
    "Visualize the training and validation losses during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b3f7a-3c62-4347-85ec-24c942bca135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca1463-f5be-4323-b6ec-a555e670d78d",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Next, let's use our trained CNN that we've loaded and saved to the variable `cnn_model` to generate predictions (labels and probabilities) on the testing set images.\n",
    "\n",
    "**Note:** Since we're using the binary cross-entropy loss, the outputs from the forward pass are already converted to probabilities (thanks to PyTorch).\n",
    "\n",
    "**A.** Create empty lists for the variables `test_pred_probs` and `test_pred_labels` to save the predicted probabilities and labels.\n",
    "\n",
    "**B.** Within the `torch.no_grad()` statement:\n",
    "- loop through the testing images and labels in the testing dataloader iterable\n",
    "- pass the images through the forward pass of the CNN to generate outputs (probabilities)\n",
    "- add the probabilities to the list `test_pred_probs`\n",
    "    - **Hint:** Use `test_pred_probs.extend(outputs.cpu().numpy())`\n",
    "- use the `torch.round()` function to round the probabilities to their predicted labels\n",
    "    - **Hint:** probabilities `>.50` are assigned label `1`, else they are assigned label `0`\n",
    "- add the predicted labels to the list `test_pred_labels`\n",
    "    - - **Hint:** Use `test_pred_labels.extend(pred_labels.cpu().numpy())`\n",
    "     \n",
    "**C.** Convert `test_pred_probs` and `test_pred_labels` to NumPy arrays using `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fde736-4407-48ac-b0a3-11f656371b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_pred_probs = []\n",
    "test_pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Get model outputs (already has sigmoid activation)\n",
    "        outputs = cnn_model(images)\n",
    "        \n",
    "        # Store probabilities\n",
    "        test_pred_probs.extend(outputs.cpu().numpy())\n",
    "        \n",
    "        # Get predicted labels (threshold at 0.5)\n",
    "        pred_labels = torch.round(outputs)\n",
    "        test_pred_labels.extend(pred_labels.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays for easier handling\n",
    "test_pred_probs = np.array(test_pred_probs)\n",
    "test_pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "# Show output\n",
    "print(\"First image predicted probability:\", test_pred_probs[0])\n",
    "print(\"First image predicted label:\", test_pred_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fde04-b2ae-427c-b46e-d165baed9a5e",
   "metadata": {},
   "source": [
    "### Task 12 \n",
    "\n",
    "Let's evaluated the trained CNN by generating a classification report.\n",
    "\n",
    "**A.** Obtain the testing set labels within the testing set dataloader.\n",
    "- initialize an empty list `test_true_labels = []` to store the true labels\n",
    "- loop through the images and labels in `test_dataloader` and use `.extend()` to add each label (as a NumPy array to the `test_true_labels` list\n",
    "\n",
    "**B.** Create a list named `pcam_classes` that stores the PCam class name\n",
    "- index `0` should reference the `'Normal'` class\n",
    "- index `1` should reference the `'Tumor'` class\n",
    "\n",
    "**C.** Generate a classification report using the predicted and true labels. Save the report to the variable `report`.\n",
    "- be sure to format the report by passing the PCam classes into the parameter `target_names=`\n",
    "\n",
    "Print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f97c3-c629-470d-99f5-46f1d47a9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_true_labels = []\n",
    "for _, labels in test_dataloader:\n",
    "    test_true_labels.extend(labels.numpy())\n",
    "\n",
    "# Define class names for PCam\n",
    "pcam_classes = ['Normal', 'Tumor']\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true=test_true_labels, \n",
    "                               y_pred=test_pred_labels,\n",
    "                               target_names=pcam_classes)\n",
    "\n",
    "# show report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0701229-9f06-43cc-a49f-d0992e20d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dcf2c64-3a71-405d-bc5b-c0b100a2ee33",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Nice! Congrats on finishing the project on building a CNN to detect tumors within images of metastatic tissue in the PatchCamelyon (PCam) dataset! There's definitely a lot of room for improvement and we encourage you to use your skills to explore different techniques to enhance the vision models.\n",
    "\n",
    "Here are some areas for improvement:\n",
    "try experimenting with different augmetation techniques (it may be useful to research techniques that are specific to medical imaging)\n",
    "- explore different model architectures in the CNN (adding additional layers or changing the parameters of the convolution layers)\n",
    "- test different loss functions or optimizers with varying learning rates\n",
    "\n",
    "While we've built a model for detecting tumors, this project primarily serves as an educational foundation for understanding how deep learning models can be applied to medical images. \n",
    "\n",
    "In the real-world, medical AI systems  require extensive testing before applying them to real patients. For example, here are just a few crucial things to consider:\n",
    "\n",
    "- optimizing for accuracy may not always be appropriate\n",
    "- understanding the trade-offs between precision and recall (false positives and false negatives)\n",
    "- robustness across different imaging conditions\n",
    "- the importance of model interpretability for clinical trust among practitioners and patients\n",
    "- the importance of consulting with medical professionals to provide valuable insights with their expertise\n",
    "er memory.\n",
    "\n",
    "Happy coding!ing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3159e7c-bea5-4082-8258-4e536027a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a2596-cd9e-427d-98f9-21975e5a5713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
