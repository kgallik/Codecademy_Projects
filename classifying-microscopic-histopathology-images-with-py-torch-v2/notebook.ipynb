{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc16985f-ec8c-434c-90b6-54227e04e2ea",
   "metadata": {},
   "source": [
    "# Classifying Microscopic Histopathology Images with PyTorch\n",
    "\n",
    "- [View Solution Notebook](./solutions.html)\n",
    "- [View Project Page](https://www.codecademy.com/content-items/b68fc937824d450e8bce11e24126e1e8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2c653-6127-4917-9084-de9f46c606d3",
   "metadata": {},
   "source": [
    "**Setup - Libraries and Custom PCam Dataset Loading Class**\n",
    "\n",
    "Run the cell below to import the libraries and the custom PCam dataset loading class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bba83e27-89d4-459f-8eda-5045f5f6f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "torch.manual_seed(42)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class PCamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading the microscopic histopathology images within the PCam dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, transform=None, num_samples=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            num_samples (int, optional): Number of samples to load. If None, loads all samples\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        if num_samples is not None:\n",
    "            self.annotations = self.annotations.head(num_samples)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        # Get image path and label\n",
    "        img_path = self.annotations.iloc[idx, 0]\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "        \n",
    "        # Load and convert image\n",
    "        image = Image.open(img_path)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Convert label to float\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17292ac3-37e5-47c3-8dc7-4ecc9b4063f8",
   "metadata": {},
   "source": [
    "## Task Group 1 - CNN Pre-Processing\n",
    "\n",
    "Let's first train a CNN model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8adfaec-b2b0-458f-8e7c-f8a17206eccf",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Let's use `transforms.Compose([])` to create the image pre-processing pipeline to apply the following transformations **and** augmentations to the training set:\n",
    "- resizes images to `96`x`96` pixels\n",
    "- randomly flips images horizontally\n",
    "- randomly rotates images `15` degrees clockwise or counterclockwise\n",
    "- adjusts brightness and contrast within 20% (or `0.20`)\n",
    "- converts the image datatype to a PyTorch tensor with values ranging from `[0.0, 1.0]`\n",
    "- normalize the pixel values within the **3** color channels to have a mean of `0.5` and a standard deviation of `0.5`\n",
    "\n",
    "Save the training set pre-processing pipeline to the variable `train_transform`.\n",
    "\n",
    "**Note**: be sure to apply the transformations in the correct order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d39828-d35c-45c2-9905-e51ffe62fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((96,96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(0.2),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a4210-0f88-414c-9213-9945013d938d",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Next, load the PCam **training set** while applying the training transformation pipeline to each image using the custom `PCamDataset` class with the following parameters:\n",
    "- `csv_file` to specify the training set located in the path `'data/train_labels.csv'`\n",
    "- `transform` to apply the `train_transform` pre-processing pipeline\n",
    "\n",
    "Save the PCam training set to the variable `train_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275a16fa-6b83-41b1-a4b2-de73ed537fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PCamDataset(csv_file='data/train_labels.csv',transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484e8e1-8d30-4d12-80d3-08f26352cfc7",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Create an iterable using the PyTorch `DataLoader` utility class that allows us to efficiently load the training set images in batches during training:\n",
    "\n",
    "- set to load `8` training images per batch\n",
    "- be sure to shuffle the training images\n",
    "\n",
    "Save the dataloader iterable to the variable `train_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137129f4-9f5d-4bfb-ba97-f8b121092589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c61af-5ff8-47c9-83d2-2c06530f95cf",
   "metadata": {},
   "source": [
    "### Optional Task\n",
    "\n",
    "Visualize the images in the first training batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608422eb-6361-4cc0-9d93-11aad13d25a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36bd0803-4a69-4eb1-8407-46f744ee19aa",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Next, let's load and pre-process the PCam **validation** and **testing** set.\n",
    "\n",
    "Remember, we don't need to apply the augmentation techniques used to pre-process the training set. But we do need to apply the same transformations (resizing, normalization, and tensor conversion) to ensure the testing set images are consistent with the training set images and are suitable for use as model inputs.\n",
    "\n",
    "**A.** Create the image pre-processing pipeline using the `transforms.Compose([])` class from the `torchvision` library that applies the following transformations to the testing set:\n",
    "- resizes images to `96`x`96` pixels\n",
    "- converts the image datatype to a PyTorch tensor with values ranging from `[0.0, 1.0]`\n",
    "- normalize the pixel values within the 3 color channels to have a mean of `0.5` and a standard deviation of `0.5`\n",
    "\n",
    "Save the validation/testing pre-processing pipeline to the variable `val_test_transform`.\n",
    "\n",
    "**Note**: be sure to apply the transformations in the correct order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acee7571-49a1-423d-b616-a49308038207",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((96,96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379cbd2-53f5-49de-98c9-6be6312b7afa",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "**A.** Next, load the PCam **validation set** while applying the validation/testing pre-processing pipeline:\n",
    "- `csv_file` to specify the validation set located in the path `'data/validation_labels.csv'`\n",
    "- `transform` to apply the `val_test_transform` pre-processing pipeline\n",
    "\n",
    "Save the loaded validation set to the variable `val_dataset`.\n",
    "\n",
    "**B.** Lastly, create an iterable using the PyTorch `DataLoader` utility class that allows us to efficiently load the validation set images in batches during evaluation:\n",
    "\n",
    "- set to load `32` images per batch\n",
    "- be sure **not** to shuffle the images\n",
    "\n",
    "Save the dataloader iterable to the variable `val_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8457913-a888-45e0-9866-dd48d9d05499",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PCamDataset(csv_file='data/validation_labels.csv',transform=val_test_transform)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d11ae01-d3d6-4955-8e07-7c75373b8e2d",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "**A.** Next, load the PCam **testing set** while applying the validation/testing pre-processing pipeline:\n",
    "- `csv_file` to specify the validation set located in the path `'data/test_labels.csv'`\n",
    "- `transform` to apply the `val_test_transform` pre-processing pipeline\n",
    "\n",
    "Save the loaded validation set to the variable `test_dataset`.\n",
    "\n",
    "**B.** Lastly, create an iterable using the PyTorch `DataLoader` utility class that allows us to efficiently load the testing set images in batches during evaluation:\n",
    "\n",
    "- set to load `32` images per batch\n",
    "- be sure **not** to shuffle the images\n",
    "\n",
    "Save the dataloader iterable to the variable `test_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc16cf2-6a1b-4d33-b548-74312dcb6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PCamDataset(csv_file='data/test_labels.csv',transform=val_test_transform)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf2a0a-9776-4ab7-83e6-d625f42518d6",
   "metadata": {},
   "source": [
    "## Task Group 2 - CNN Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7075e8-9ce7-4afc-af33-e79a936b6c0e",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Let's now a create a CNN architecture as a class named `SimpleCNN` using the `nn.Module`.\n",
    "\n",
    "**A.** Define the `__init__` method with the following CNN layers:\n",
    "1. `self.conv1` is the first convolutional layer with `3` input channels, `32` output channels, `3x3` filter sizes, and `1` padding\n",
    "2. `self.conv2` is the second convolutional layer with `32` input channels, `64` output channels, `3x3` filter sizes, and `1` padding\n",
    "3. `self.conv3` is the third convolutional layer with `64` input channels, `128` output channels, `3x3` filter sizes, and `1` padding \n",
    "4. `self.fc1` is the first fully connected layer with `18432` input nodes and `256` output nodes\n",
    "    - `18432` corresponds to the length of the flattened 1-D vector after last max pooling layer (`128 x 12 x 12 = 18432`)\n",
    "5. `self.fc2` is the second fully connected layer with `256` input nodes and `1` output node\n",
    "\n",
    "**B.** Define the `forward` method that processes each image `x` with the forward operations in following order:\n",
    "\n",
    "1. Pass the image through the first convolutional layer and then apply the ReLU activation function\n",
    "2. Pass the first activated convoluted output to a max pooling layer with a 2x2 filter\n",
    "3. Pass the first max pooling output to the second convolutional layer and then apply the ReLU activation function\n",
    "4. Pass the second activated convoluted output to a 2nd max pooling layer with a 2x2 filter\n",
    "5. Pass the second max pooling output to the third convolutional layer and then apply the ReLU activation function\n",
    "6. Pass the third activated convoluted output to a 3rd max pooling layer with a 2x2 filter\n",
    "7. Flatten the third max pooling output into a tensor (with batch size)\n",
    "8. Pass the flattened tensor to the first fully connected layer and then apply the ReLU activation function\n",
    "9. Pass the activated output to the second fully connected layer through a Sigmoid activation function\n",
    "    - **Hint:** Use `x = torch.sigmoid(self.fc2(x)).squeeze(1)`\n",
    "10. Return Sigmoid activated output\n",
    "\n",
    "**C.** Create an instance of the CNN model class and save it to the variable `cnn_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56456311-31c7-436a-bf04-6a63e16d2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.fc1 = nn.Linear(18432,256)\n",
    "        self.fc2 = nn.Linear(256,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x,kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x,kernel_size=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x,kernel_size=2)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x)).squeeze(1)\n",
    "        return x\n",
    "\n",
    "cnn_model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb7e21-f261-4d99-b7e9-c768d6b41211",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Set the CNN hardware device to GPU/CPU.\n",
    "\n",
    "**A.** Create the `device` variable that detects whether GPU (`'cuda'`) or CPU is available.\n",
    "- use `torch.device()` to detect the device\n",
    "- use `torch.cuda.is_available()` to check if GPU is available\n",
    "    - if GPU is available, return the string `'cuda'`\n",
    "    - if not, return the string `'cpu'`\n",
    "\n",
    "**B.** Move the `cnn_model` to the available device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f851e43d-e6b6-4028-8458-980ad666fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707a95e-d73e-442e-b980-b9dab910541e",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Let's initialize the loss function and optimizer for training using the `torch.optim` module.\n",
    "\n",
    "**A.** Create an instance of the binary-entropy loss function `nn.BCELoss()` in PyTorch and save it to the variable `criterion`.\n",
    "\n",
    "**B.** Create an instance of the Adam optimizer in PyTorch with a learning rate of `0.0005` and save it to the variable `optimizer`.\n",
    "- be sure to optimize the parameters in the CNN model we instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "345bc867-f34c-46f4-8c8e-6437fab0c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: BCELoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(),lr=0.0005)\n",
    "\n",
    "print(\"Loss function:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3968c-0aaf-4d08-abaf-7b85c2ad345a",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Now, let's create a training loop that:\n",
    "- trains the CNN model on training set and keeps track of the training loss\n",
    "- keeps track of the validation loss on the training set\n",
    "\n",
    "**A.** Initialize the following empty lists to keep track the training and validation losses per epoch:\n",
    "- `train_losses` to keep track of the training loss\n",
    "- `val_losses` to keep track of the validation loss\n",
    "\n",
    "**B.** Train the CNN on `5` epochs by assigning the value `5` to the variable `num_epochs`.\n",
    "\n",
    "**C.** Initialize the following variables to `0` to keep track of during training:\n",
    "- `total_train_loss` to keep track of the total training loss\n",
    "- `total_val_loss` to keep track of the total validation loss\n",
    "\n",
    "**D.** Build the training section:\n",
    "- loop through the training batch images and labels\n",
    "- within each training batch:\n",
    "    - place the images to the GPU device \n",
    "    - reset the gradients to zero\n",
    "    - pass the batch through the CNN forward pass\n",
    "    - calculate the training loss\n",
    "    - backpropagate the loss\n",
    "    - adjust the weights and biases\n",
    "    - update the total training loss\n",
    "\n",
    "**E.** Within the validation section, evaluate the validation set performance:\n",
    "- loop through the validation batch images and labels\n",
    "- within each validation batch:\n",
    "    -  place the images to the GPU device\n",
    "    -  pass the batch through the CNN forward pass\n",
    "    -  calculate the validation loss\n",
    "    -  update the total validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "944c165c-86e6-4897-89bc-65cf914168a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: Training Loss: 0.0525 - Validation Loss: 0.5624\n",
      "Epoch [2/5]: Training Loss: 0.0527 - Validation Loss: 0.5646\n",
      "Epoch [3/5]: Training Loss: 0.0484 - Validation Loss: 0.5189\n",
      "Epoch [4/5]: Training Loss: 0.0489 - Validation Loss: 0.5241\n",
      "Epoch [5/5]: Training Loss: 0.0456 - Validation Loss: 0.4888\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    #training\n",
    "    cnn_model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = cnn_model(images)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_train_loss = total_val_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "     # Print training and validation losses\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b92a4f-516a-4b86-b2da-a040e5109fab",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Hint: Tracking the training and validation losses</summary>\n",
    "    \n",
    "You can add this code snippet to the training loop to track the training and validation losses:\n",
    "\n",
    "```py\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    # Save the average training and validation losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Print training and validation losses\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39daf771-f520-4582-8d51-3719ca26d794",
   "metadata": {},
   "source": [
    "### Optional Task\n",
    "\n",
    "Visualize the training and validation losses during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b3f7a-3c62-4347-85ec-24c942bca135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cca1463-f5be-4323-b6ec-a555e670d78d",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Next, let's use our trained CNN that we've loaded and saved to the variable `cnn_model` to generate predictions (labels and probabilities) on the testing set images.\n",
    "\n",
    "**Note:** Since we're using the binary cross-entropy loss, the outputs from the forward pass are already converted to probabilities (thanks to PyTorch).\n",
    "\n",
    "**A.** Create empty lists for the variables `test_pred_probs` and `test_pred_labels` to save the predicted probabilities and labels.\n",
    "\n",
    "**B.** Within the `torch.no_grad()` statement:\n",
    "- loop through the testing images and labels in the testing dataloader iterable\n",
    "- pass the images through the forward pass of the CNN to generate outputs (probabilities)\n",
    "- add the probabilities to the list `test_pred_probs`\n",
    "    - **Hint:** Use `test_pred_probs.extend(outputs.cpu().numpy())`\n",
    "- use the `torch.round()` function to round the probabilities to their predicted labels\n",
    "    - **Hint:** probabilities `>.50` are assigned label `1`, else they are assigned label `0`\n",
    "- add the predicted labels to the list `test_pred_labels`\n",
    "    - - **Hint:** Use `test_pred_labels.extend(pred_labels.cpu().numpy())`\n",
    "     \n",
    "**C.** Convert `test_pred_probs` and `test_pred_labels` to NumPy arrays using `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25fde736-4407-48ac-b0a3-11f656371b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_probs = []\n",
    "test_pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cnn_model(images)\n",
    "        test_pred_probs.extend(outputs.cpu().numpy())\n",
    "        pred_labels = torch.round(outputs)\n",
    "        test_pred_labels.extend(pred_labels.cpu().numpy())\n",
    "    test_pred_labels = np.array(test_pred_labels)\n",
    "    test_pred_probs = np.array(test_pred_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fde04-b2ae-427c-b46e-d165baed9a5e",
   "metadata": {},
   "source": [
    "### Task 12 \n",
    "\n",
    "Let's evaluated the trained CNN by generating a classification report.\n",
    "\n",
    "**A.** Obtain the testing set labels within the testing set dataloader.\n",
    "- initialize an empty list `test_true_labels = []` to store the true labels\n",
    "- loop through the images and labels in `test_dataloader` and use `.extend()` to add each label (as a NumPy array to the `test_true_labels` list\n",
    "\n",
    "**B.** Create a list named `pcam_classes` that stores the PCam class name\n",
    "- index `0` should reference the `'Normal'` class\n",
    "- index `1` should reference the `'Tumor'` class\n",
    "\n",
    "**C.** Generate a classification report using the predicted and true labels. Save the report to the variable `report`.\n",
    "- be sure to format the report by passing the PCam classes into the parameter `target_names=`\n",
    "\n",
    "Print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e79f97c3-c629-470d-99f5-46f1d47a9dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.84      0.80      0.82        97\n",
      "       Tumor       0.82      0.85      0.84       103\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.83      0.83      0.83       200\n",
      "weighted avg       0.83      0.83      0.83       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_true_labels = []\n",
    "for images,labels in test_dataloader:\n",
    "    test_true_labels.extend(labels.numpy())\n",
    "\n",
    "pcam_classes = ['Normal','Tumor']\n",
    "\n",
    "report = classification_report(test_true_labels, test_pred_labels, target_names=pcam_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0701229-9f06-43cc-a49f-d0992e20d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dcf2c64-3a71-405d-bc5b-c0b100a2ee33",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Nice! Congrats on finishing the project on building a CNN to detect tumors within images of metastatic tissue in the PatchCamelyon (PCam) dataset! There's definitely a lot of room for improvement and we encourage you to use your skills to explore different techniques to enhance the vision models.\n",
    "\n",
    "Here are some areas for improvement:\n",
    "try experimenting with different augmetation techniques (it may be useful to research techniques that are specific to medical imaging)\n",
    "- explore different model architectures in the C (adding additional layers or changing the parameters of the convolution layers)ng\n",
    "- test different loss functions or optimizers with varying learning rate\n",
    "\n",
    "aWhile we've built a model for detecting tumors, this project primarily serves as an educational foundation for understanding how deep learning models can be applied to medical images. \n",
    "\n",
    "In the real-world, medical AI systems  require extensive testing before applying them to real patients. For example, here are just a few crucial things to consider:\n",
    "\n",
    "- optimizing for accuracy may not always be appropriate\n",
    "- understanding the trade-offs between precision and recall (false positives and false negatives)\n",
    "- robustness across different imaging conditions\n",
    "- the importance of model interpretability for clinical trust among practitioners and patients\n",
    "- the importance of consulting with medical professionals to provide valuable insights with their expertise\n",
    "er memory.\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3159e7c-bea5-4082-8258-4e536027a6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Equation for calculating the output size from a convolution/pooling layer:\n",
    "# O = (I - K + 2P)/S + 1 \n",
    "(32-3+2*1)//2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b7593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBook-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
